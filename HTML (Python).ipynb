{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web scraping\n",
    "\n",
    "When there are no APIs, data can be collected from website directly using techniques such as web scraping. This means extracting from the HTML content directly. This can be text, images, or, links.\n",
    "\n",
    "To do this, one needs to parse HTML markup (text) into meaningful structure. In Python [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/) is a popular library to work on HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install BeautifulSoup4\n",
    "!pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "website = requests.get(\"https://www.helsinki.fi\").text\n",
    "parsed = BeautifulSoup( website, 'html.parser' ) ## parser, lxml might be faster sometimes\n",
    "\n",
    "print( parsed )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding elements\n",
    "\n",
    "Command `find_all` can be used to find all spesific elements from the website, either using their tag, their ID or their CSS class, or combinations of these.\n",
    "Command `find` could be used to find only one element.\n",
    "It is also possible to nest these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for link in parsed.find_all('a'):\n",
    "    print( link )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in parsed.find_all( class_ = 'paragraph'):\n",
    "    print( text )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in parsed.find_all( id = 'does_not_exist'):\n",
    "    print( text )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in parsed.find_all(attrs={\"role\": \"banner\"}):\n",
    "    print( text )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## nested structure\n",
    "\n",
    "for banner in parsed.find_all(attrs={\"role\": \"banner\"}):\n",
    "    for link in banner.find_all('a'):\n",
    "        print( link )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attributes and content\n",
    "\n",
    "HTML tags have attributes and content and one can access them through parsing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for link in parsed.find_all('a'):\n",
    "    print( link.get('href'), link.get_text() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "\n",
    "1. Find all links on Yle.fi main page. What amount of them starts with http?\n",
    "1. Find all images on Yle.fi and print their URLs\n",
    "1. Go through all Finnish university web frontpages. Which of them have have a link to (a) Facebook, (b) TikTok or (c) X?\n",
    "1. Extract the text of a single article on Yle.fi\n",
    "1. Extract the text of a single article on HS.fi\n",
    "1. Extract the text of a single news article on Helsinki.fi\n",
    "1. Extract the text of a single news article on Aamulehti.fi\n",
    "1. Extract the text of a single news article on BBC.com\n",
    "1. Extract the text of a single news article on New York Times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XPath\n",
    "\n",
    "There is a [dedicated query language](https://en.wikipedia.org/wiki/XPath) to work with HTML/XML structured documents, you can copy the exact queries from the browser developer tools.\n",
    "This requires parsing and working with the library using lxml library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "\n",
    "dom = etree.HTML(website)\n",
    "\n",
    "for element in dom.xpath('//a'):\n",
    "    print( element.get('href'), element.text )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Following links, crawlers and spiders\n",
    "\n",
    "Sometimes there is a need to follow links, for example identify follow-up pages and crawl the content on them as well.\n",
    "On most simples format, one can detect links and open them.\n",
    "There are also spesific libraries for this purpose, such as [Scrapy](https://docs.scrapy.org/en/latest/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site = 'https://www.helsinki.fi'\n",
    "\n",
    "html = requests.get( site ).text\n",
    "parsed = BeautifulSoup( html )\n",
    "\n",
    "for link in parsed.find_all('a'):\n",
    "    if link.get('href').startswith('/'): ## checking that the link is under this main site, not e.g. to Sisu or Facebook\n",
    "        print( site + link.get('href') )\n",
    "        html2 = requests.get( site + link.get('href') ).text\n",
    "        ## parse and work forward with html2 if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "\n",
    "* Fix the above code to work correctly, i.e. manage the Nonetype challenge\n",
    "* Collect all links which are not in the same domain\n",
    "* Parse collected websites and calculate the times `cat` is mentioned throughout them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "class ToScrapeSpider(scrapy.Spider):\n",
    "    name = \"toscrape\"\n",
    "    start_urls = [\n",
    "        'http://quotes.toscrape.com/',\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        for quote in response.css(\"div.quote\"):\n",
    "            yield {\n",
    "                'text': quote.css(\"span.text::text\").extract_first(),\n",
    "                'author': quote.css(\"small.author::text\").extract_first()\n",
    "            }\n",
    "\n",
    "        next_page_url = response.css(\"li.next > a::attr(href)\").extract_first()\n",
    "        if next_page_url is not None:\n",
    "            yield scrapy.Request(response.urljoin(next_page_url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "process = CrawlerProcess(\n",
    "    settings={\n",
    "        \"FEEDS\": {\n",
    "            \"quotes.json\": {\"format\": \"json\"},\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "process.crawl( ToScrapeSpider )\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task\n",
    "\n",
    "* Adapt the scrapy parser to work on HS.fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
